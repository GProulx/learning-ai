{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ListName      ScreenName                 Name  \\\n",
      "0       ai         egrefen  Edward Grefenstette   \n",
      "1       ai  goodfellow_ian       Ian Goodfellow   \n",
      "2       ai     stanfordnlp   Stanford NLP Group   \n",
      "3       ai         NandoDF     Nando de Freitas   \n",
      "4       ai       shakir_za       Shakir Mohamed   \n",
      "\n",
      "                                         Description  PctFollowersFromList  \\\n",
      "0  French/American AI researcher at @DeepMindAI, ...              0.277778   \n",
      "1  Google Brain research scientist Lead author of...              0.611111   \n",
      "2  Computational Linguistics—Natural Language—Mac...              0.277778   \n",
      "3  I research intelligence to understand what we ...              0.388889   \n",
      "4  Research Scientist in Machine Learning and AI....              0.333333   \n",
      "\n",
      "   NbRelations  IsListRelated  \n",
      "0            5              1  \n",
      "1           11              1  \n",
      "2            5              1  \n",
      "3            7              1  \n",
      "4            6              1  \n",
      "       PctFollowersFromList  NbRelations  IsListRelated\n",
      "count             55.000000    55.000000      55.000000\n",
      "mean               0.357576     6.436364       0.709091\n",
      "std                0.097853     1.761351       0.458368\n",
      "min                0.277778     5.000000       0.000000\n",
      "25%                0.277778     5.000000       0.000000\n",
      "50%                0.333333     6.000000       1.000000\n",
      "75%                0.388889     7.000000       1.000000\n",
      "max                0.611111    11.000000       1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ia_data = pd.read_csv('../datasets/twitter-ia/ai.csv')\n",
    "print(ia_data.head())\n",
    "print(ia_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    egrefen Edward Grefenstette French/American AI...\n",
       "1    goodfellow_ian Ian Goodfellow Google Brain res...\n",
       "2    stanfordnlp Stanford NLP Group Computational L...\n",
       "3    NandoDF Nando de Freitas I research intelligen...\n",
       "4    shakir_za Shakir Mohamed Research Scientist in...\n",
       "Name: FullDescription, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia_data['FullDescription'] = ia_data['ScreenName'].str.cat(ia_data['Name'], sep=' ').str.cat(ia_data['Description'], sep=' ', na_rep=' ').str.strip()\n",
    "ia_data['FullDescription'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-8e8ca21cbd55>:24: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-8e8ca21cbd55>:30: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8e8ca21cbd55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m   \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m   \u001b[0mnormalized_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m   \u001b[0mvalid_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m   \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "embedding_size = 100  # Dimension of the embedding vector.\n",
    "vocabulary_size = 10000\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "  # Look up embeddings for inputs.\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "  # Construct the variables for the softmax\n",
    "  weights = tf.Variable(tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "    \n",
    "  # convert train_context to a one-hot format\n",
    "  train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "\n",
    "  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
